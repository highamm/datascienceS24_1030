---
title: "Section 14: k-nearest-neighbors"
author: "Matt Higham"
format: 
  html:
    embed-resources: true
---

## Section 14.2: Choosing predictors and choosing k

```{r}
set.seed(1119) ## ensures we all get the same training/test split
library(tidyverse)
library(here) 

pokemon <- read_csv(here("data", "pokemon_full.csv")) |>
  filter(Type %in% c("Steel", "Dark", "Fire", "Ice")) |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                                 (max(.x) - min(.x)))) 
## scaling the numeric predictor variables
## where(is.numeric) says to only do this scaling
## for numeric variables (<dbl> or <int>)
```

```{r}
## putting 75 poikemon (randomly selected) into the
## training sample
train_sample <- pokemon |>
  slice_sample(n = 75)

test_sample <- anti_join(pokemon, train_sample)
## test_sample is any pokemon not in the training_sample
```

```{r}
## choosing predictors
library(GGally)
ggpairs(data = train_sample, columns = c(4, 5, 6, 3))
## columns as HP, Attack, Defense, Type
## Type is the response and usually goes last

## Defense looks like the best predictor though Attack and
## HP also look useful (looking at the last column for
## "separation" in the boxplots as evidence for a "good"
## predictor)
```

```{r}
ggpairs(data = train_sample, columns = c(7, 8, 12, 3))
## all predictors might be useful as there is some separation
## in each set of boxplots
```

```{r}
## install.packages("class")
library(class)

## create a data frame that only has the predictors
## that we will use
train_small <- train_sample |> select(HP, Attack, Defense, Speed,
                                      SpAtk) ## data frame that only has our chosen predictors
test_small <- test_sample |> select(HP, Attack, Defense, Speed,
                                    SpAtk) ## data frame that has only the same set of predictors

## put our response variable into a vector
train_cat <- train_sample$Type ## training sample response
test_cat <- test_sample$Type ## test sample response

## k is the number of nearest neighbors
knn_mod <- knn(train = train_small, test = test_small,
               cl = train_cat, k = 11)
knn_mod
## vector of predictions for the pokemon in the test sample
## ie. the first pokemon is predicted to be Fire
## the second is also predicted to be Fire, etc
```

Evaluating the model

```{r}
table(knn_mod, test_cat) 
## all of the correct classifications are on the diagonal
## off-diagonal elements contain all of the misclassifications

## exercises 2-3
## for example, 11 fire pokemon were correctly predicted to be fire
## for example, 3 fire pokemon were incorrectly predicted to be dark
```

Primary Metric: Classification Rate

```{r}
(1 + 11 + 1 + 3) / 45

## classification rate is 0.35556
## larger classification rate <=> better model
```

Exericse 5. Automatically compute the classification rate

```{r}
tab <- table(knn_mod, test_cat) 
sum(diag(tab)) / sum(tab)
```

Exercise 6. Adding SpAtk or changing k to 11 or both was useful: my classification rate increased!

Exercise 7. Compute Baseline Classification Rate

```{r}
## figure out which level of Type is most common in the training sample? (using code)
train_sample |> group_by(Type) |>
  summarise(n_type = n()) |>
  arrange(desc(n_type))
## with the test sample, figure out the proportion of pokemon that
## match the type that is most common in the training sample
test_sample |> mutate(is_fire = if_else(Type == "Fire",
                                        true = 1, false = 0)) |>
  relocate(is_fire) |>
  summarise(bsae_class_rate = mean(is_fire))
## our baseline classification rate is 0.356 so we hope
## our model will do much better than this!
```

Exercise 8. A larger k means that you are using more data points (but potentially introducing more bias). A smaller k means that you are only using observations most similar to the test case (but potentially introduces more variability).

Class Exercise 1.

```{r}
library(tidyverse)

pokemon <- read_csv(here::here("data/pokemon_full.csv")) 
set.seed(1119)

## scale the quantitative predictors
pokemon_scaled <- pokemon |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                  (max(.x) - min(.x))))

train_sample <- pokemon_scaled |>
  slice_sample(n = 550)
test_sample <- anti_join(pokemon_scaled, train_sample)

library(class)

train_pokemon <- train_sample |> select(HP, Attack, Defense, Speed,
                                        SpAtk, SpDef, height, weight)
test_pokemon <- test_sample |> select(HP, Attack, Defense, Speed,
                                      SpAtk, SpDef, height, weight)

## put our response variable into a vector
train_cat <- train_sample$Type
test_cat <- test_sample$Type

knn_mod <- knn(train = train_pokemon, test = test_pokemon,
               cl = train_cat, k = 6)
knn_mod

tab <- table(knn_mod, test_cat)
sum(diag(tab)) / sum(tab)
```

```{r}
get_class_rate <- function(k_val) {
  knn_mod <- knn(train = train_pokemon, test = test_pokemon,
                 cl = train_cat, k = k_val)
  knn_mod
  
  tab <- table(knn_mod, test_cat)
  class_rate <- sum(diag(tab)) / sum(tab)
  
  return(class_rate)
}

get_class_rate(k_val = 12)
```

Mapping through our function:

```{r}
k_vec <- 1:30
k_vec

## first argument is a vector of values we want to map through
## second argument is the function that we want to plug
## each of those values into

## vector of classification rates:
class_rates <- map_dbl(k_vec, get_class_rate)

class_rate_df <- tibble(k_vec, class_rates)
ggplot(data = class_rate_df, aes(x = k_vec, y = class_rates)) +
  geom_point() +
  geom_line()
## k = 19 maximizes the classification rate:

class_rate_df |> filter(class_rates == max(class_rates))

```
